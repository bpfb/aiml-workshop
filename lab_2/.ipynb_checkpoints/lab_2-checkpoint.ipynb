{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reinforcement Learning: Hello World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using AWS SageMaker for RL\n",
    "\n",
    "AWS SageMaker allows you to train your RL agents in cloud machines using docker containers. You do not have to worry about setting up your machines with the RL toolkits and deep learning frameworks. You can easily switch between many different machines setup for you, including powerful GPU machines that give a big speedup. You can also choose to use multiple machines in a cluster to further speedup training, often necessary for production level loads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites \n",
    "\n",
    "#### Imports\n",
    "\n",
    "To get started, we'll import the Python libraries we need, set up the environment with a few prerequisites for permissions and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import subprocess\n",
    "from IPython.display import HTML\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "sys.path.append(\"common\")\n",
    "from misc import get_execution_role, wait_for_s3_object\n",
    "from sagemaker.rl import RLEstimator, RLToolkit, RLFramework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Settings\n",
    "\n",
    "Let's define a few configuration settings and values we will use later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 bucket path: s3://sagemaker-us-east-1-273210948404/\n"
     ]
    }
   ],
   "source": [
    "env_type = 'simple-corridor'\n",
    "\n",
    "# create unique job name \n",
    "job_name_prefix = 'rl-' + env_type\n",
    "\n",
    "# S3 bucket\n",
    "sage_session = sagemaker.session.Session()\n",
    "s3_bucket = sage_session.default_bucket()  \n",
    "s3_output_path = 's3://{}/'.format(s3_bucket)\n",
    "print(\"S3 bucket path: {}\".format(s3_output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an IAM role\n",
    "Either get the execution role when running from a SageMaker notebook `role = sagemaker.get_execution_role()` or, when running from local machine, use utils method `role = get_execution_role('role_name')` to create an execution role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using IAM role arn: arn:aws:iam::273210948404:role/onica-aiml-workshop-ExecutionRole-U2CRSU9PTJ0Y\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except:\n",
    "    role = get_execution_role()\n",
    "\n",
    "print(\"Using IAM role arn: {}\".format(role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the environment\n",
    "\n",
    "The environment is defined in a Python file called “simple_corridor_env.py” and the file is uploaded on /src directory. \n",
    "\n",
    "The environment also implements the init(), step(), reset() and render() functions that describe how the environment behaves. This is consistent with Open AI Gym interfaces for defining an environment. \n",
    "\n",
    "\n",
    "1. Init() - initialize the environment in a pre-defined state\n",
    "2. Step() - take an action on the environment\n",
    "3. reset()- restart the environment on a new episode\n",
    "4. render() - get a rendered image of the environment in its current state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the presets for RL algorithm \n",
    "\n",
    "The presets that configure the RL training jobs are defined in the “preset-tsp-easy.py” file which is also uploaded on the /src directory. Using the preset file, you can define agent parameters to select the specific agent algorithm. You can also set the environment parameters, define the schedule and visualization parameters, and define the graph manager. The schedule presets will define the number of heat up steps, periodic evaluation steps, training steps between evaluations.\n",
    "\n",
    "These can be overridden at runtime by specifying the RLCOACH_PRESET hyperparameter. Additionally, it can be used to define custom hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.agents.clipped_ppo_agent\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ClippedPPOAgentParameters\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.environments.gym_environment\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m GymVectorEnvironment\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.graph_managers.basic_rl_graph_manager\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BasicRLGraphManager\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.graph_managers.graph_manager\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SimpleSchedule\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.core_types\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\r\n",
      "\u001b[37m#from rl_coach.environments.environment import SelectedPhaseOnlyDumpMethod, MaxDumpMethod\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m logger\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.base_parameters\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TaskParameters\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.base_parameters\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m VisualizationParameters\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.architectures.embedder_parameters\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m InputEmbedderParameters\r\n",
      "\r\n",
      "\u001b[37m################\u001b[39;49;00m\r\n",
      "\u001b[37m#  Environment #\u001b[39;49;00m\r\n",
      "\u001b[37m################\u001b[39;49;00m\r\n",
      "\r\n",
      "env_params = GymVectorEnvironment(level=\u001b[33m'\u001b[39;49;00m\u001b[33msimple_corridor_env:SimpleCorridor\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[37m#########\u001b[39;49;00m\r\n",
      "\u001b[37m# Agent #\u001b[39;49;00m\r\n",
      "\u001b[37m#########\u001b[39;49;00m\r\n",
      "\r\n",
      "agent_params = ClippedPPOAgentParameters()\r\n",
      "\r\n",
      "\u001b[37m#################\u001b[39;49;00m\r\n",
      "\u001b[37m# Visualization #\u001b[39;49;00m\r\n",
      "\u001b[37m#################\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m#env_params.frame_skip = 5 #to make sure the gifs work without skipping steps\u001b[39;49;00m\r\n",
      "\r\n",
      "vis_params = VisualizationParameters()\r\n",
      "vis_params.dump_gifs=\u001b[36mFalse\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m####################\u001b[39;49;00m\r\n",
      "\u001b[37m# Graph Scheduling #\u001b[39;49;00m\r\n",
      "\u001b[37m####################\u001b[39;49;00m\r\n",
      "\r\n",
      "schedule_params=SimpleSchedule()\r\n",
      "schedule_params.improve_steps = TrainingSteps(\u001b[34m10000\u001b[39;49;00m)\r\n",
      "schedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(\u001b[34m20\u001b[39;49;00m)\r\n",
      "schedule_params.evaluation_steps = EnvironmentEpisodes(\u001b[34m5\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "graph_manager = BasicRLGraphManager(\r\n",
      "    agent_params= agent_params,\r\n",
      "    env_params=env_params,\r\n",
      "    schedule_params=schedule_params,\r\n",
      "    vis_params=vis_params\r\n",
      "    )\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/preset-simple-corridor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the Training Code \n",
    "\n",
    "The training code is written in the file “train-coach.py” which is uploaded in the /src directory. \n",
    "First import the environment files and the preset files, and then define the main() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_rl.coach_launcher\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SageMakerCoachPresetLauncher\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mMyLauncher\u001b[39;49;00m(SageMakerCoachPresetLauncher):\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mdefault_preset_name\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[33m\"\"\"This points to a .py file that configures everything about the RL job.\u001b[39;49;00m\r\n",
      "\u001b[33m        It can be overridden at runtime by specifying the RLCOACH_PRESET hyperparameter.\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mpreset-simple-corridor\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mmap_hyperparameter\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, name, value):\r\n",
      "        \u001b[33m\"\"\"Here we configure some shortcut names for hyperparameters that we expect to use frequently.\u001b[39;49;00m\r\n",
      "\u001b[33m        Essentially anything in the preset file can be overridden through a hyperparameter with a name \u001b[39;49;00m\r\n",
      "\u001b[33m        like \"rl.agent_params.algorithm.etc\".  \u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "        \u001b[37m# maps from alias (key) to fully qualified coach parameter (value)\u001b[39;49;00m\r\n",
      "        mapping = {\r\n",
      "                      \u001b[33m\"\u001b[39;49;00m\u001b[33mdiscount\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.agent_params.algorithm.discount\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "                      \u001b[33m\"\u001b[39;49;00m\u001b[33mevaluation_episodes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.evaluation_steps:EnvironmentEpisodes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "                      \u001b[33m\"\u001b[39;49;00m\u001b[33mimprove_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.improve_steps:TrainingSteps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "                  }\r\n",
      "        \u001b[34mif\u001b[39;49;00m name \u001b[35min\u001b[39;49;00m mapping:\r\n",
      "            \u001b[36mself\u001b[39;49;00m.apply_hyperparameter(mapping[name], value)\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            \u001b[36msuper\u001b[39;49;00m().map_hyperparameter(name, value)\r\n",
      "    \r\n",
      "    \r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "    MyLauncher.train_main()\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/train-coach-simple-corridor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the RL model using the Python SDK Script mode\n",
    "\n",
    "If you are using local mode, the training will run on the notebook instance. When using SageMaker for training, you can select a GPU or CPU instance. The RLEstimator is used for training RL jobs. \n",
    "\n",
    "1. Specify the source directory where the environment, presets and training code is uploaded.\n",
    "2. Specify the entry point as the training code \n",
    "3. Specify the choice of RL toolkit and framework. This automatically resolves to the ECR path for the RL Container. \n",
    "4. Define the training parameters such as the instance count, job name, S3 path for output and job name. \n",
    "5. Specify the hyperparameters for the RL agent algorithm. The RLCOACH_PRESET or the RLRAY_PRESET can be used to specify the RL agent algorithm you want to use. \n",
    "6. Define the metrics definitions that you are interested in capturing in your logs. These can also be visualized in CloudWatch and SageMaker Notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 102 ms, sys: 7.51 ms, total: 110 ms\n",
      "Wall time: 433 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "estimator = RLEstimator(entry_point=\"train-coach-simple-corridor.py\",\n",
    "                        source_dir='src',\n",
    "                        dependencies = [\"common/sagemaker_rl\"],\n",
    "                        toolkit=RLToolkit.COACH,\n",
    "                        toolkit_version='0.11',\n",
    "                        framework=RLFramework.TENSORFLOW,\n",
    "                        role=role,\n",
    "                        train_instance_type='ml.m4.4xlarge',\n",
    "                        train_instance_count=1,\n",
    "                        output_path=s3_output_path,\n",
    "                        base_job_name=job_name_prefix,\n",
    "                        hyperparameters = {\n",
    "                          #expected run time 12 mins for TSP Easy  \n",
    "                          \"RLCOACH_PRESET\": \"preset-\" + env_type, \n",
    "                        }\n",
    "                    )\n",
    "\n",
    "estimator.fit(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store intermediate training output and model checkpoints \n",
    "\n",
    "The output from the training job above is stored on S3. The intermediate folder contains gifs and metadata of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job name: rl-simple-corridor-2019-06-26-23-34-10-674\n",
      "S3 job path: s3://sagemaker-us-east-1-273210948404/rl-simple-corridor-2019-06-26-23-34-10-674\n",
      "Output.tar.gz location: s3://sagemaker-us-east-1-273210948404/rl-simple-corridor-2019-06-26-23-34-10-674/output/output.tar.gz\n",
      "Intermediate folder path: s3://sagemaker-us-east-1-273210948404/rl-simple-corridor-2019-06-26-23-34-10-674/output/intermediate\n",
      "Create local folder /tmp/rl-simple-corridor-2019-06-26-23-34-10-674\n"
     ]
    }
   ],
   "source": [
    "job_name=estimator._current_job_name\n",
    "\n",
    "print(\"Job name: {}\".format(job_name))\n",
    "\n",
    "s3_url = \"s3://{}/{}\".format(s3_bucket,job_name)\n",
    "\n",
    "output_tar_key = \"{}/output/output.tar.gz\".format(job_name)\n",
    "\n",
    "intermediate_folder_key = \"{}/output/intermediate\".format(job_name)\n",
    "output_url = \"s3://{}/{}\".format(s3_bucket, output_tar_key)\n",
    "intermediate_url = \"s3://{}/{}\".format(s3_bucket, intermediate_folder_key)\n",
    "\n",
    "print(\"S3 job path: {}\".format(s3_url))\n",
    "print(\"Output.tar.gz location: {}\".format(output_url))\n",
    "print(\"Intermediate folder path: {}\".format(intermediate_url))\n",
    "    \n",
    "tmp_dir = \"/tmp/{}\".format(job_name)\n",
    "os.system(\"mkdir {}\".format(tmp_dir))\n",
    "print(\"Create local folder {}\".format(tmp_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing against a baseline policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot metrics for training job\n",
    "We can pull the reward metric of the training and plot it to see the performance of the model over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for s3://sagemaker-us-east-1-273210948404/rl-simple-corridor-2019-06-26-23-34-10-674/output/intermediate/worker_0.simple_rl_graph.main_level.main_level.agent_0.csv.........."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "# csv_file has all the RL training metrics\n",
    "# csv_file = \"{}/worker_0.simple_rl_graph.main_level.main_level.agent_0.csv\".format(tmp_dir)\n",
    "csv_file_name = \"worker_0.simple_rl_graph.main_level.main_level.agent_0.csv\"\n",
    "key = intermediate_folder_key + \"/\" + csv_file_name\n",
    "wait_for_s3_object(s3_bucket, key, tmp_dir)\n",
    "\n",
    "csv_file = \"{}/{}\".format(tmp_dir, csv_file_name)\n",
    "df = pd.read_csv(csv_file)\n",
    "x_axis = 'Episode #'\n",
    "y_axis_rl = 'Training Reward'\n",
    "df[y_axis_rl] = df[y_axis_rl].rolling(50).mean()\n",
    "ax = df.plot(x=x_axis,y=y_axis_rl, figsize=(18,6), fontsize=18, legend=True, color='b')\n",
    "fig = ax.get_figure()\n",
    "ax.set_xlabel(x_axis,fontsize=20)\n",
    "#ax.set_ylabel(y_axis,fontsize=20)\n",
    "#fig.savefig('training_reward_vs_wall_clock_time.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of RL models\n",
    "\n",
    "We use the last checkpointed model to run evaluation for the RL Agent. \n",
    "\n",
    "#### Load checkpointed model\n",
    "\n",
    "Checkpointed data from the previously trained models will be passed on for evaluation / inference in the checkpoint channel. In local mode, we can simply use the local directory, whereas in the SageMaker mode, it needs to be moved to S3 first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "wait_for_s3_object(s3_bucket, output_tar_key, tmp_dir)  \n",
    "\n",
    "if not os.path.isfile(\"{}/output.tar.gz\".format(tmp_dir)):\n",
    "    raise FileNotFoundError(\"File output.tar.gz not found\")\n",
    "os.system(\"tar -xvzf {}/output.tar.gz -C {}\".format(tmp_dir, tmp_dir))\n",
    "\n",
    "\n",
    "checkpoint_dir = \"{}/checkpoint\".format(tmp_dir)\n",
    "\n",
    "print(\"Checkpoint directory {}\".format(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "checkpoint_path = \"s3://{}/{}/checkpoint/\".format(s3_bucket, job_name)\n",
    "if not os.listdir(checkpoint_dir):\n",
    "    raise FileNotFoundError(\"Checkpoint files not found under the path\")\n",
    "os.system(\"aws s3 cp --recursive {} {}\".format(checkpoint_dir, checkpoint_path))\n",
    "print(\"S3 checkpoint file path: {}\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the evaluation step\n",
    "\n",
    "Use the checkpointed model to run the evaluation step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_eval = RLEstimator(role=role,\n",
    "                             source_dir='src/',\n",
    "                             dependencies = [\"common/sagemaker_rl\"],\n",
    "                             toolkit=RLToolkit.COACH,\n",
    "                             toolkit_version='0.11.0',\n",
    "                             framework=RLFramework.TENSORFLOW,\n",
    "                             entry_point=\"evaluate-coach.py\",\n",
    "                             train_instance_count=1,\n",
    "                             train_instance_type='ml.m4.4xlarge',\n",
    "                             hyperparameters = {\n",
    "                                 \"RLCOACH_PRESET\": \"preset-simple-corridor\",\n",
    "                                 \"evaluate_steps\": 200, #max 4 episodes\n",
    "                             }\n",
    "                            )\n",
    "\n",
    "estimator_eval.fit({'checkpoint': checkpoint_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
