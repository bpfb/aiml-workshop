{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplying Oil Well Sites\n",
    "This notebook demonstrates the processing of data collected by IoT sensors at multiple oil wells, for the purpose of prioritizing resupply of each well site.  Various chemicals are consumed at each well site at different rates and need to be replenished when running low.  In order to minimize resupply travel and costs, a prioritization algorithm is developed to yield the sites and routing of resupply trucks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sagemaker import get_execution_role\n",
    "from  more_itertools import unique_everseen\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Discovery\n",
    "\n",
    "Let's take a look at the data and get a handle on what we're looking at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('onica3.csv')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the first few rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different rows appear to contain different information and populate different columns.  We're going to have to clean this up before we can do much with it.  Let's check how many different kinds of rows there are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(unique_everseen(raw_data['point']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many sites and assets (chemical types):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(unique_everseen(raw_data['site'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(unique_everseen(raw_data['asset'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(unique_everseen(zip(raw_data['site'],raw_data['asset']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL (Extract, Transform and Load)\n",
    "The times are in the file as Unix epoch time (here, milliseconds from 1970-01-01); we'll turn them into datetime objects for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['rollupStartTimestampMs'] = pd.to_datetime(raw_data['rollupStartTimestampMs'], unit='ms')\n",
    "raw_data['rollupEndTimestampMs'] = pd.to_datetime(raw_data['rollupEndTimestampMs'], unit='ms')\n",
    "raw_data['latestObservationTimestamp'] = pd.to_datetime(raw_data['latestObservationTimestamp'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the data rows are attributes of the site or asset, such as the latitude/longitude or chemical name.  We need to separate the timeseries data from these attribute data, and drop the columns not used in the timeseries data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts = raw_data[(raw_data.point == 'PRODUCT_VOLUME') | (raw_data.point == 'EST_DAYS_TO_EMPTY')| \n",
    "                 (raw_data.point == 'AVERAGE_CHEMICAL_DAILY_USAGE') | (raw_data.point == 'PERCENT_FULL')]\n",
    "\n",
    "# drop attribute-related columns\n",
    "del df_ts['latestObservationTimestamp']\n",
    "del df_ts['latestObservation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now extract the static attribute data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attributes = raw_data[(raw_data.point == 'LATITUDE') | (raw_data.point == 'LONGITUDE')| \n",
    "                         (raw_data.point == 'TANK_VOLUME') | (raw_data.point == 'CHEMICAL_NAME')]\n",
    "\n",
    "#Drop timeseries-related columns\n",
    "del df_attributes['rollupStartTimestampMs']\n",
    "del df_attributes['rollupEndTimestampMs']\n",
    "del df_attributes['numericSumValue']\n",
    "del df_attributes['numericMinValue']\n",
    "del df_attributes['numericMaxValue']\n",
    "del df_attributes['numericLastValue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attributes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attributes.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform some pandas pivot table magic to rearrange the data frame, and ensure the data are the right data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attributes = pd.pivot_table(df_attributes, \n",
    "                               index=['site','asset'], \n",
    "                               values=['latestObservation'],\n",
    "                               columns=['point'], \n",
    "                               aggfunc=lambda x: ' '.join(map(str, x)))\n",
    "# get rid of multi-level index\n",
    "df_attributes = df_attributes.xs('latestObservation', axis=1, drop_level=True)\n",
    "df_attributes = df_attributes.reset_index()\n",
    "\n",
    "# convert the data with numeric data to the right data type\n",
    "numeric_cols = ['LATITUDE','LONGITUDE','TANK_VOLUME']\n",
    "df_attributes[numeric_cols] = df_attributes[numeric_cols].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attributes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge timeseries data frame and attribute data frame.  Note that it's an inner join, so we're dropping sites with no timeseries information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_ts.merge(df_attributes, how = 'inner', on = ['site','asset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary value appears to be the `PRODUCT_VOLUME`, and the others are derived from that.  Let's look at a few sites timeseries data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sites(sites, df, ycol='numericLastValue'):\n",
    "    for site in sites:\n",
    "        df_site = df[df['site']==site]\n",
    "        for cname, group in df_site.groupby('CHEMICAL_NAME'):\n",
    "            ax = group.plot(x='rollupEndTimestampMs', y=ycol, title=str(site), marker='+', label=cname)\n",
    "            ax.set_xlabel('Time')\n",
    "            ax.set_ylabel('Volume')\n",
    "            #print(site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_list = list(unique_everseen(df['site']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample = 5\n",
    "variable = 'PERCENT_FULL' #'PRODUCT_VOLUME'\n",
    "\n",
    "df_variable = df[df['point']==variable].copy()\n",
    "\n",
    "plot_sites(np.random.choice(site_list, num_sample), df_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chemicals at most sites follow a logical pattern of steady depletion with occasional fill events rapidly increasing chemical volume levels.  Here are a few with a \"typical\" pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_sites = [2539767100, 2543205088, 2546199776, 2615146580, 2550863632]\n",
    "plot_sites(normal_sites, df_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are some timeseries which don't make sense, like the reported volume dropping to zero, or a single value in the timeseries.  These are likely sensor failures of some kind.  Other sites show slow increases in volume, which seems strange but could be from thermal expansion of the chemicals in the tank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unusual_sites = [2612863532, 2552756476, 2545207496, 2616812000] #, 2552369860, 2553594584, 2539931224]\n",
    "plot_sites(unusual_sites, df_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empty Tank Prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize time stamp as 'date' to capture all movement within one day and allow for grouping/sorting if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_variable['date'] = df_variable['rollupEndTimestampMs'].dt.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_variable.sort_values(['site','asset','rollupEndTimestampMs'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIXME: this doesn't account for time deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_variable['rollupEndTimestampMs'] - df_variable['rollupEndTimestampMs'].shift()).apply(lambda x: x.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solve for daily percent full delta for any give site and asset\n",
    "\n",
    "df_variable['percent_full_delta'] = np.where(df_variable['site'] == df_variable['site'].shift(), df_variable['numericLastValue'] - df_variable['numericLastValue'].shift() , np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multi(sites, df, ycol=['numericLastValue','percent_full_delta']):\n",
    "    for site in sites:\n",
    "        df_site = df[df['site']==site]\n",
    "        for cname, group in df_site.groupby('CHEMICAL_NAME'):\n",
    "            ax = group.plot(x='rollupEndTimestampMs', y=ycol, title=str(site), marker='+', label=[cname,'delta'])\n",
    "            ax.set_xlabel('Time')\n",
    "            ax.set_ylabel('Volume')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multi([2546199776,2543205088,2553849160], df_variable) #, ycol='percent_full_delta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 10\n",
    "df_variable['fill_event'] = df_variable['percent_full_delta'] > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_rate_of_depletion = df_variable[df_variable['fill_event']==False].groupby(['site','asset'], as_index=False).agg({\"percent_full_delta\":\"mean\"})\n",
    "df_mean_rate_of_depletion.rename(columns={'percent_full_delta':'mean_rate_of_depletion'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_rate_of_depletion.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Apply a rate of depletion to the current chemical levels to predict time to empty.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull latest percent full levels by site and asset\n",
    "\n",
    "df_fill_prediction = df_variable.groupby(['site','asset'], as_index=False).agg({\"rollupEndTimestampMs\":\"max\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ties with depletion rate count, great!\n",
    "\n",
    "df_fill_prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge lateset time stamp to be predicted on with rates of depletion\n",
    "\n",
    "df_fill_prediction = pd.merge(df_fill_prediction, df_mean_rate_of_depletion, on=['site','asset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fill_prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add other available fields at the latest timestamp for each site,asset\n",
    "\n",
    "df_fill_prediction = pd.merge(df_fill_prediction, df_variable, on=['site','asset','rollupEndTimestampMs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data frame clean up\n",
    "\n",
    "# df_fill_prediction.drop('point', axis=1, inplace=True)\n",
    "# df_fill_prediction.drop('numericSumValue', axis=1, inplace=True) \n",
    "# df_fill_prediction.drop('numericMinValue', axis=1, inplace=True)\n",
    "# df_fill_prediction.drop('numericMaxValue', axis=1, inplace=True)\n",
    "# df_fill_prediction.drop('rollupStartTimestampMs', axis=1, inplace=True)\n",
    "# df_fill_prediction.drop('fill_event', axis=1, inplace=True)\n",
    "# df_fill_prediction.drop('percent_full_delta', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fill_prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caclulate predicted days to empty\n",
    "\n",
    "df_fill_prediction['days_to_empty'] = df_fill_prediction['numericLastValue'] / -(df_fill_prediction['mean_rate_of_depletion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fill_prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fill_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Forecast the date at which the tank will reach empty.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fill_prediction = df_fill_prediction.replace([np.inf, -np.inf], np.nan).dropna(subset=[\"days_to_empty\"], how=\"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, drop the pathological rows.  In production, these would be flagged for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_idx = df_fill_prediction[(df_fill_prediction['mean_rate_of_depletion']>=0) | (df_fill_prediction['mean_rate_of_depletion']<-10)].index\n",
    "df_fill_prediction.drop(bad_idx, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fill_prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(df_fill_prediction['mean_rate_of_depletion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fill_prediction['mean_rate_of_depletion'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=df_fill_prediction.plot(x='mean_rate_of_depletion', y='days_to_empty', linewidth=0, marker='+')\n",
    "a.set_ylim(0,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Filter for working sensors and prioritize site and chemical fill requirements by time to empty.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering (K-Means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***K-means on location and n = 5 clusters:***\n",
    "https://www.naftaliharris.com/blog/visualizing-k-means-clustering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 6371.0 #-- mean radius of curvatureof Earth (km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat0 = np.median(df_fill_prediction['LATITUDE'])\n",
    "lon0 = np.median(df_fill_prediction['LONGITUDE'])\n",
    "print(lat0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fill_prediction['dN'] = np.radians(df_fill_prediction['LATITUDE'] - lat0) * R\n",
    "df_fill_prediction['dE'] = np.radians(df_fill_prediction['LONGITUDE'] - lon0) * R * np.cos(np.radians(lat0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#Make a copy of DF\n",
    "#must_fill = 30  #-- days\n",
    "#df_tr = df_fill_prediction[df_fill_prediction['days_to_empty'] <= must_fill].copy()\n",
    "df_tr = df_fill_prediction.copy()\n",
    "\n",
    "#Standardize\n",
    "#clmns = ['LONGITUDE', 'LATITUDE']\n",
    "clmns = ['dE', 'dN']\n",
    "df_tr_std = stats.zscore(df_tr[clmns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster the data\n",
    "n_clusters = 5\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(df_tr_std)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "#Glue back to original data\n",
    "df_tr['cluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarize clustering results\n",
    "\n",
    "df_tr[['site','asset','cluster']].groupby(['cluster']).agg(['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize clustering results; scatter plot of LONGITUDE and LATITUDE\n",
    "sns.lmplot('dE','dN',\n",
    "           data=df_tr, \n",
    "           fit_reg=False, \n",
    "           hue=\"cluster\",  \n",
    "           scatter_kws={\"marker\": \"D\", \n",
    "                        \"s\": 100, \"alpha\": 0.3})\n",
    "plt.title('Site Location Clusters')\n",
    "plt.xlabel('Kilometers East')\n",
    "plt.ylabel('Kilometers North')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Identify minimum days to empty to prioritize fill schedule by cluster:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group data by clusters and indentify the minimum days to empty for each cluster\n",
    "\n",
    "df_cluster_min = df_tr.groupby(['cluster'], as_index=False).agg({\"days_to_empty\":\"min\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename the days_to_empty comlumn to prepare merge with df_fill_prediciton\n",
    "\n",
    "df_cluster_min = df_cluster_min.rename(columns={'days_to_empty':'min_days_to_empty'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Review\n",
    "\n",
    "df_cluster_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm for \"Traveling Salesman Problem\" (TSP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the euclidian distance in n-space of the route r traversing cities (sites) c, ending at the path start.\n",
    "path_distance = lambda r,c: np.sum([np.linalg.norm(c[r[p]]-c[r[p-1]]) for p in range(len(r))])\n",
    "\n",
    "# Reverse the order of all elements from element i to element k in array r.\n",
    "two_opt_swap = lambda r,i,k: np.concatenate((r[0:i],r[k:-len(r)+i-1:-1],r[k+1:len(r)]))\n",
    "\n",
    "def two_opt(cities,improvement_threshold): # 2-opt Algorithm adapted from https://en.wikipedia.org/wiki/2-opt\n",
    "    route = np.arange(cities.shape[0]) # Make an array of row numbers corresponding to cities (sites).\n",
    "    improvement_factor = 1 # Initialize the improvement factor.\n",
    "    best_distance = path_distance(route,cities) # Calculate the distance of the initial path.\n",
    "    while improvement_factor > improvement_threshold: # If the route is still improving, keep going!\n",
    "        distance_to_beat = best_distance # Record the distance at the beginning of the loop.\n",
    "        for swap_first in range(1,len(route)-2): # From each city (site) except the first and last,\n",
    "            for swap_last in range(swap_first+1,len(route)): # to each of the cities (sites) following,\n",
    "                new_route = two_opt_swap(route,swap_first,swap_last) # try reversing the order of these cities (sites)\n",
    "                new_distance = path_distance(new_route,cities) # and check the total distance with this modification.\n",
    "                if new_distance < best_distance: # If the path distance is an improvement,\n",
    "                    route = new_route # make this the accepted best route\n",
    "                    best_distance = new_distance # and update the distance corresponding to this route.\n",
    "        improvement_factor = 1 - best_distance/distance_to_beat # Calculate how much the route has improved.\n",
    "    return route # When the route is no longer improving substantially, stop searching and return the route.\n",
    "\n",
    "# Source: https://stackoverflow.com/questions/25585401/travelling-salesman-in-scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_min.sort_values('min_days_to_empty', inplace=True)\n",
    "df_cluster_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df_cluster_min['cluster'].values:\n",
    "    df_sites = df_tr[df_tr['cluster']==c]\n",
    "    sites = df_sites[['dE','dN']].values\n",
    "    \n",
    "    # Find a good route with 2-opt (\"route\" gives the order in which to travel to each site by row number.)\n",
    "    route = two_opt(sites,0.001)\n",
    "    \n",
    "    # Plot the recommended path\n",
    "    ax = (df_sites.iloc[route]).plot(x='dE',y='dN', marker='D', title='Path for Cluster {}'.format(c))\n",
    "    ax.set_xlabel('km East')\n",
    "    ax.set_ylabel('km North')\n",
    "    ax.get_legend().remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
